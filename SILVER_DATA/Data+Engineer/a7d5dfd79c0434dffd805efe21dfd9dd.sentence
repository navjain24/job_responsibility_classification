You will:
   
    
      Collaborate on the design and improvements of the data infrastructure
      Partner with operations, product, and engineering to advocate best practices and build supporting systems and infrastructure for the various data needs
      Create data pipelines that stitch together various data sources in order to produce valuable data products that drive the business
      Create real-time data pipelines and build models using tools like python and DBT
    
   
  
  
 
  
   You will love this role if you have:
   
    
      3+ years of experience in a data engineering role building products, ideally in a fast-paced environment
      Good foundations in Python or another object oriented language
      Experience with Spark, PySpark, Glue, SQL, DBT and Redshift
      Experience building high volume log ingestion and storage pipelines
      Experience with AWS, Azure, or GCP security ecosystem and tooling
      Experience working with containerization.